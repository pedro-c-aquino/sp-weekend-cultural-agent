import json
from .planner import Planner
from .actor import Actor
from .evaluator import Evaluator
from .parser import Parser
from ..tools.scraper import scrape_pages
from ..llm import LLM
from ..tools.calendar import current_weekend


class Orchestrator:
    def __init__(self, model: str = "llama3.1:8b-instruct"):
        self.llm = LLM(provider="ollama", model=model)
        self.planner = Planner(self.llm)
        self.actor = Actor()
        self.evaluator = Evaluator(self.llm)
        self.parser = Parser(self.llm)

    async def weekend_run(self, focus: str, mode: str = "serp"):
        fri, sun = current_weekend()
        weekend_range = f"{fri.date()} to {sun.date()}"
        plan = await self.planner.plan(
            f"Eventos de {fri.date()} a {sun.date()} em SÃ£o Paulo;"
        )
        print(
            "================================ PLAN ===================================\n",
            plan,
        )
        # for q in queries:
        #     serp.extend(self.actor.websearch(q, k=12, timelimit="w"))

        # # keep a compact payload
        # results = []
        # seen = set()
        # for r in serp:
        #     url = r.get("href") or r.get("url")
        #     if not url or url in seen:
        #         continue
        #     seen.add(url)
        #     results.append(
        #         {
        #             "title": r.get("title"),
        #             "snippet": r.get("body"),
        #             "url": url,
        #             "source": r.get("source"),
        #             "date": r.get("date"),
        #         }
        #     )
        #     if len(results) >= 30:
        #         break
        # print("==================== SERP Results =========================\n", results)
        # print("Starting scrapping")
        # pages = await scrape_pages(r["url"] for r in results)
        # print("==================== SCRAPE RESULTS ==========================\n", pages)
        # print("Start parsing")

        # url_to_events = {}
        # for page in pages:
        #     events = await self.parser.parse(
        #         text=page["text"],
        #         page_url=page["url"],
        #         weekend_range=weekend_range,
        #         focus=focus,
        #     )
        #     url_to_events[page["url"]] = events
        # print(
        #     "================================ PARSING RESULTS ===================================\n",
        #     url_to_events,
        # )
        # for r in results:
        #     r["events_on_page"] = url_to_events.get(r["url"], [])

        # all_events = []
        # for r in results:
        #     evs = r.get("events_on_page") or []
        #     all_events.extend(evs)

        # score = self.evaluator.score(all_events)

        # evaluation =  await self.evaluator.evaluate(results)
        # evaluation_serialized = [e.model_dump() for e in evaluation]

        # return {"queries": queries, "results": results, "score": score.model_dump(), "evaluation": evaluation_serialized}
